{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827d0fc-8f68-4eba-8de0-fbbbaa11cda0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T08:08:48.293942Z",
     "iopub.status.busy": "2023-08-31T08:08:48.293484Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-31 16:08:51,548] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "2023-08-31 16:08:51.812508: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "08/31/2023 16:08:52 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "[INFO|configuration_utils.py:667] 2023-08-31 16:08:52,723 >> loading configuration file /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-08-31 16:08:52,724 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49954\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-08-31 16:08:52,724 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-08-31 16:08:52,724 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-08-31 16:08:52,724 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-08-31 16:08:52,724 >> loading file tokenizer_config.json\n",
      "08/31/2023 16:08:53 - INFO - datasets.builder - Using custom data configuration default-bb1f620ea08b00b9\n",
      "08/31/2023 16:08:53 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/json\n",
      "08/31/2023 16:08:53 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/31/2023 16:08:53 - INFO - datasets.info - Loading Dataset info from /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\n",
      "08/31/2023 16:08:53 - WARNING - datasets.builder - Found cached dataset json (/mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "08/31/2023 16:08:53 - INFO - datasets.info - Loading Dataset info from /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 676.03it/s]\n",
      "08/31/2023 16:08:53 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ce990a6f075b8688_00000_of_00004.arrow\n",
      "08/31/2023 16:08:53 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ce990a6f075b8688_00001_of_00004.arrow\n",
      "08/31/2023 16:08:53 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ce990a6f075b8688_00002_of_00004.arrow\n",
      "08/31/2023 16:08:53 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ce990a6f075b8688_00003_of_00004.arrow\n",
      "08/31/2023 16:08:53 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Running tokenizer on dataset (num_proc=4):   0%| | 0/82600 [00:00<?, ? examples/08/31/2023 16:08:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ce990a6f075b8688_00000_of_00004.arrow\n",
      "08/31/2023 16:08:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ce990a6f075b8688_00001_of_00004.arrow\n",
      "08/31/2023 16:08:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ce990a6f075b8688_00002_of_00004.arrow\n",
      "08/31/2023 16:08:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ce990a6f075b8688_00003_of_00004.arrow\n",
      "08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-46ecc0fa49ea2f67_00000_of_00004.arrow\n",
      "08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-46ecc0fa49ea2f67_00001_of_00004.arrow\n",
      "08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-46ecc0fa49ea2f67_00002_of_00004.arrow\n",
      "08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-46ecc0fa49ea2f67_00003_of_00004.arrow\n",
      "08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Running tokenizer on dataset (num_proc=4):   0%| | 0/7656 [00:00<?, ? examples/s08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-46ecc0fa49ea2f67_00000_of_00004.arrow\n",
      "08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-46ecc0fa49ea2f67_00002_of_00004.arrow\n",
      "08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-46ecc0fa49ea2f67_00001_of_00004.arrow\n",
      "08/31/2023 16:09:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-46ecc0fa49ea2f67_00003_of_00004.arrow\n",
      "08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6cc0543c6781e59d_00000_of_00004.arrow\n",
      "08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6cc0543c6781e59d_00001_of_00004.arrow\n",
      "08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6cc0543c6781e59d_00002_of_00004.arrow\n",
      "08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6cc0543c6781e59d_00003_of_00004.arrow\n",
      "08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Running tokenizer on dataset (num_proc=4):   0%| | 0/7656 [00:00<?, ? examples/s08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6cc0543c6781e59d_00000_of_00004.arrow\n",
      "08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6cc0543c6781e59d_00001_of_00004.arrow\n",
      "08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6cc0543c6781e59d_00002_of_00004.arrow\n",
      "08/31/2023 16:09:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6cc0543c6781e59d_00003_of_00004.arrow\n",
      "08/31/2023 16:09:37 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "tokenized_dataset:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 82600\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7656\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7656\n",
      "    })\n",
      "})\n",
      "[1, 29871, 31658, 30383, 13, 32237, 31138, 32380, 36655, 30214, 39313, 34339, 36655, 30275, 34534, 30210, 33942, 37110, 30214, 31666, 37887, 38920, 31149, 39990, 30952, 30383, 13, 36655, 32342, 30383, 13, 32822, 30383, 32593, 41049, 31639, 32269, 43434, 43095, 30210, 32027, 30882, 13, 32822, 30383, 38020, 33096, 30806, 36021, 32822, 31999, 38020, 32764, 32066, 32161, 32058, 30882, 13, 34339, 36655, 30383, 13, 33062, 30383, 31238, 32292, 30850, 32980, 33673, 34982, 43095, 13, 30682, 31333, 39990, 30952, 41133, 30383, 32009, 42147, 31751, 33942, 30214, 42147, 31751, 33942, 30214, 32395, 32237, 35230, 30333, 32932, 35358, 32262, 42147, 31751, 33942, 13, 33091, 30383, 13, 34339, 36655, 32204, 33942, 33858, 39990, 30952, 33933, 30573, 30383, 13, 43095, 30383, 32009, 42147, 31751, 33942, 2]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 34339, 36655, 32204, 33942, 33858, 39990, 30952, 33933, 30573, 30383, 13, 43095, 30383, 32009, 42147, 31751, 33942, 2]\n",
      "08/31/2023 16:09:37 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-53f07414ab2782f9_00000_of_00004.arrow\n",
      "08/31/2023 16:09:37 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-53f07414ab2782f9_00001_of_00004.arrow\n",
      "08/31/2023 16:09:37 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-53f07414ab2782f9_00002_of_00004.arrow\n",
      "08/31/2023 16:09:37 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-53f07414ab2782f9_00003_of_00004.arrow\n",
      "08/31/2023 16:09:37 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-53f07414ab2782f9_*_of_00004.arrow\n",
      "08/31/2023 16:09:37 - INFO - datasets.arrow_dataset - Concatenating 4 shards\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-19b08660c5b0a54e_00000_of_00004.arrow\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-19b08660c5b0a54e_00001_of_00004.arrow\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-19b08660c5b0a54e_00002_of_00004.arrow\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-19b08660c5b0a54e_00003_of_00004.arrow\n",
      "08/31/2023 16:09:38 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-19b08660c5b0a54e_*_of_00004.arrow\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Concatenating 4 shards\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-203d597da0723f71_00000_of_00004.arrow\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-203d597da0723f71_00001_of_00004.arrow\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-203d597da0723f71_00002_of_00004.arrow\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-203d597da0723f71_00003_of_00004.arrow\n",
      "08/31/2023 16:09:38 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-bb1f620ea08b00b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-203d597da0723f71_*_of_00004.arrow\n",
      "08/31/2023 16:09:38 - INFO - datasets.arrow_dataset - Concatenating 4 shards\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 36974\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3800\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3800\n",
      "    })\n",
      "})\n",
      "08/31/2023 16:09:38 - INFO - __main__ - Num train_samples  36974\n",
      "08/31/2023 16:09:38 - INFO - __main__ - training example:\n",
      "08/31/2023 16:09:38 - INFO - __main__ - <s> 问：\n",
      "以下文本包含医疗三元组，请找出并填写：\n",
      "化疗对脉络丛乳头状癌也有效果，术前化疗尚可减少肿瘤体积及血液供应，有助于手术全切。有作者统计12例，5年生存率75%，10年生存率66. 6%。\n",
      "其中的三元组类型是：病理生理，相关（转化），放射治疗，药物治疗，实验室检查，化疗，多发季节，治疗后症状，影像学检查，死亡率，发病部位，并发症，筛查，预后生存率，内窥镜检查，侵及周围组织转移的症状，传播途径，组织学检查，预防，多发地区，相关（症状），临床表现，辅助检查，预后状况，相关（导致），发病年龄，手术治疗，风险评估因素，转移部位，病理分型，同义词，高危因素，发病率，外侵部位，多发群体，发病性别倾向，就诊科室\n",
      "答：\n",
      "具有化疗关系的头尾实体对如下：头实体为脉络丛乳头状癌，尾实体为化疗。头实体为脉络丛乳头状癌，尾实体为术前化疗。\n",
      "具有预后生存率关系的头尾实体对如下：头实体为脉络丛乳头状癌，尾实体为5年生存率75%。头实体为脉络丛乳头状癌，尾实体为10年生存率66. 6%。\n",
      "具有手术治疗关系的头尾实体对如下：头实体为脉络丛乳头状癌，尾实体为手术全切。</s><s>问：\n",
      "帮助患者自动总结问诊的诊疗报告：\n",
      "问诊对话历史：\n",
      "患者：医生你好，我女儿咳嗽，夜里不咳，就是一吃东西就咳得厉害，请问是什么原因？\n",
      "医生：你好\n",
      "患者：你好\n",
      "医生：孩子咳嗽几天了\n",
      "患者：有一个星期了\n",
      "医生：有发烧吗？\n",
      "医生：咳嗽有痰吗？\n",
      "患者：不发烧，有痰\n",
      "医生：去当地公立医院检查过吗？\n",
      "患者：没有，在私人诊所打过针\n",
      "医生：治疗以后效果怎么样？\n",
      "医生：咳嗽见轻吧\n",
      "患者：轻点了\n",
      "医生：但是还是咳嗽，痰比较多是吗\n",
      "08/31/2023 16:09:38 - INFO - __main__ - ？\n",
      "患者：就是吃饭时咳\n",
      "患者：痰不多\n",
      "医生：根据你说的孩子的情况。咳嗽有痰这还是呼吸道的疾病\n",
      "医生：我考虑孩子还是患有上呼吸道感染的。\n",
      "患者：哦，要吃什么药？\n",
      "医生：着凉感染以后都会引起上呼吸道感染的。\n",
      "医生：孩子现在精神好吗？吃饭可以吗？\n",
      "患者：好的\n",
      "医生：那还好根据孩子目前的情况没有发烧精神比较好，吃饭也可以。经过治疗病情也是减轻的，可以继续口服药物缓解一下。\n",
      "医生：可以适当的把药物调整一下。你现在吃的什么药，打的什么针。\n",
      "医生：经过治疗虽然病情减轻，但是还是咳嗽，这是病情还没有完全控制住的。\n",
      "医生：现在吃的什么药物？知道名字吗？\n",
      "患者：小儿止咳糖浆，头孢\n",
      "患者：不打针了\n",
      "医生：吃了几天啦？\n",
      "患者：也有四五天\n",
      "医生：你上边的药物可以继续吃的。\n",
      "患者：哦，\n",
      "医生：可以，再加上氨溴索口服液。和小儿双金清热解毒口服液\n",
      "医生：仔细看说明，按说明书吃。\n",
      "医生：这四种药物一起吃效果还是比较好的\n",
      "医生：加强护理不要着凉的不要吃辛辣的东西，多吃蔬菜和水果。\n",
      "患者：好的，谢谢\n",
      "医生：上呼吸道感染一般的，一周左右会恢复好的。\n",
      "医生：但是孩子小上呼吸道感染很容易引起气管炎和肺炎的。\n",
      "医生：继续口服药物三到四天观察变化。\n",
      "患者：哦，知道了\n",
      "医生：如果咳嗽咳痰不见好转就去当地公立医院小儿内科就诊检查。\n",
      "医生：化验血常规拍胸片看是否有气管炎和肺炎。\n",
      "医生：在采取适当的治疗措施，效果还好。\n",
      "患者：恩\n",
      "医生：如果合并气管炎和肺炎。口服药物效果是不好的，应该静脉输液效果还是比较好的。\n",
      "医生：好的继续口服药物配合，加强护理。观察病情变化如何\n",
      "医生：如果口服药物三至四天。没有咳嗽和咳痰了可以停止药物的\n",
      "医生：口服药三至四天如果没有咳嗽和咳痰，这是病情恢复了，可以停服药物的\n",
      "说明：诊疗报告分为主诉, 现病史, 辅助检查, 既往史, \n",
      "[INFO|modeling_utils.py:2265] 2023-08-31 16:09:38,140 >> Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "[INFO|modeling_utils.py:2278] 2023-08-31 16:09:38,140 >> The device_map was not initialized.Setting device_map to {'':torch.cuda.current_device()}.If you want to use the model for inference, please set device_map ='auto' \n",
      "[INFO|modeling_utils.py:2575] 2023-08-31 16:09:38,141 >> loading weights file /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1173] 2023-08-31 16:09:38,141 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:577] 2023-08-31 16:09:38,141 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2690] 2023-08-31 16:09:38,269 >> Detected 8-bit loading: activating 8-bit loading for this model\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:09<00:00,  2.27s/it]\n",
      "[INFO|modeling_utils.py:3295] 2023-08-31 16:09:47,571 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3303] 2023-08-31 16:09:47,571 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2023-08-31 16:09:47,574 >> loading configuration file /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3/generation_config.json\n",
      "[INFO|configuration_utils.py:577] 2023-08-31 16:09:47,575 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.9,\n",
      "  \"top_p\": 0.6,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "08/31/2023 16:09:47 - INFO - __main__ - Init new peft model\n",
      "['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
      "8\n",
      "trainable params: 19,988,480 || all params: 6,905,483,264 || trainable%: 0.28945809056123434\n",
      "[INFO|trainer.py:399] 2023-08-31 16:10:35,468 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "[INFO|trainer.py:407] 2023-08-31 16:10:35,468 >> The model is loaded in 8-bit precision. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check  the examples in https://github.com/huggingface/peft for more details.\n",
      "[INFO|trainer.py:577] 2023-08-31 16:10:35,468 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "**************************************************\n",
      "resume_from_checkpoint:  None\n",
      "**************************************************\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1786] 2023-08-31 16:10:35,662 >> ***** Running training *****\n",
      "[INFO|trainer.py:1787] 2023-08-31 16:10:35,662 >>   Num examples = 36,974\n",
      "[INFO|trainer.py:1788] 2023-08-31 16:10:35,662 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1789] 2023-08-31 16:10:35,662 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1790] 2023-08-31 16:10:35,662 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1791] 2023-08-31 16:10:35,662 >>   Gradient Accumulation steps = 16\n",
      "[INFO|trainer.py:1792] 2023-08-31 16:10:35,662 >>   Total optimization steps = 2,400\n",
      "[INFO|trainer.py:1793] 2023-08-31 16:10:35,667 >>   Number of trainable parameters = 19,988,480\n",
      "{'loss': 2.5521, 'learning_rate': 4.1666666666666667e-07, 'epoch': 0.0}         \n",
      "{'loss': 5.6814, 'learning_rate': 3.75e-06, 'epoch': 0.0}                       \n",
      "{'loss': 6.4698, 'learning_rate': 7.916666666666667e-06, 'epoch': 0.01}         \n",
      "{'loss': 4.0588, 'learning_rate': 1.2083333333333333e-05, 'epoch': 0.01}        \n",
      "{'loss': 4.6539, 'learning_rate': 1.6250000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.682, 'learning_rate': 2e-05, 'epoch': 0.02}                          \n",
      "{'loss': 3.4643, 'learning_rate': 2.4166666666666667e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.5506, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.03}        \n",
      "{'loss': 4.4163, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.03}        \n",
      "{'loss': 2.6665, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.04}        \n",
      "{'loss': 2.8273, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.04}        \n",
      "  4%|█▌                                    | 100/2400 [20:48<7:56:16, 12.42s/it][INFO|trainer.py:2926] 2023-08-31 16:31:24,547 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-100\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 16:31:24,650 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 16:31:24,650 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 3.0732, 'learning_rate': 4.5e-05, 'epoch': 0.05}                       \n",
      "{'loss': 3.3028, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.05}        \n",
      "{'loss': 3.2061, 'learning_rate': 4.999848114735858e-05, 'epoch': 0.06}         \n",
      "{'loss': 2.0334, 'learning_rate': 4.9992311124800875e-05, 'epoch': 0.06}        \n",
      "{'loss': 2.691, 'learning_rate': 4.9981396174548355e-05, 'epoch': 0.06}         \n",
      "{'loss': 1.7895, 'learning_rate': 4.996573836886435e-05, 'epoch': 0.07}         \n",
      "{'loss': 2.4412, 'learning_rate': 4.994534068046937e-05, 'epoch': 0.07}         \n",
      "{'loss': 2.4356, 'learning_rate': 4.99229333433282e-05, 'epoch': 0.08}          \n",
      "{'loss': 1.9573, 'learning_rate': 4.989354129000368e-05, 'epoch': 0.08}         \n",
      "{'loss': 2.3966, 'learning_rate': 4.98594230609806e-05, 'epoch': 0.09}          \n",
      "  8%|███▏                                  | 200/2400 [41:39<7:36:20, 12.45s/it][INFO|trainer.py:2926] 2023-08-31 16:52:14,950 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-200\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 16:52:15,049 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 16:52:15,049 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 2.7825, 'learning_rate': 4.982058513379235e-05, 'epoch': 0.09}         \n",
      "{'loss': 1.473, 'learning_rate': 4.9777034882032966e-05, 'epoch': 0.1}          \n",
      "{'loss': 1.6047, 'learning_rate': 4.9728780573957226e-05, 'epoch': 0.1}         \n",
      "{'loss': 2.1108, 'learning_rate': 4.967583137091085e-05, 'epoch': 0.1}          \n",
      "{'loss': 1.3295, 'learning_rate': 4.9618197325591195e-05, 'epoch': 0.11}        \n",
      "{'loss': 1.8815, 'learning_rate': 4.95558893801387e-05, 'epoch': 0.11}          \n",
      "{'loss': 1.6089, 'learning_rate': 4.948891936405941e-05, 'epoch': 0.12}         \n",
      "{'loss': 2.8919, 'learning_rate': 4.9417299991979125e-05, 'epoch': 0.12}        \n",
      "{'loss': 1.7505, 'learning_rate': 4.934104486122947e-05, 'epoch': 0.13}         \n",
      "{'loss': 1.7807, 'learning_rate': 4.9260168449266335e-05, 'epoch': 0.13}        \n",
      " 12%|████▌                               | 300/2400 [1:02:30<7:20:13, 12.58s/it][INFO|trainer.py:2926] 2023-08-31 17:13:06,531 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-300\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 17:13:06,634 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 17:13:06,634 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 2.834, 'learning_rate': 4.9174686110921245e-05, 'epoch': 0.13}         \n",
      "{'loss': 2.4479, 'learning_rate': 4.908461407548618e-05, 'epoch': 0.14}         \n",
      "{'loss': 1.9683, 'learning_rate': 4.8989969443632366e-05, 'epoch': 0.14}        \n",
      "{'loss': 1.4598, 'learning_rate': 4.889077018416359e-05, 'epoch': 0.15}         \n",
      "{'loss': 2.4734, 'learning_rate': 4.878703513060474e-05, 'epoch': 0.15}         \n",
      "{'loss': 1.4505, 'learning_rate': 4.8678783977626155e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.2044, 'learning_rate': 4.856603727730447e-05, 'epoch': 0.16}         \n",
      "{'loss': 1.9918, 'learning_rate': 4.8448816435220714e-05, 'epoch': 0.16}        \n",
      "{'loss': 4.0932, 'learning_rate': 4.832714370639633e-05, 'epoch': 0.17}         \n",
      "{'loss': 1.9295, 'learning_rate': 4.8201042191067956e-05, 'epoch': 0.17}        \n",
      " 17%|██████                              | 400/2400 [1:23:23<6:59:11, 12.58s/it][INFO|trainer.py:2926] 2023-08-31 17:33:58,688 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-400\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 17:33:58,792 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 17:33:58,792 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 1.568, 'learning_rate': 4.8070535830301664e-05, 'epoch': 0.18}         \n",
      "{'loss': 2.005, 'learning_rate': 4.793564940144769e-05, 'epoch': 0.18}          \n",
      "{'loss': 1.4293, 'learning_rate': 4.7796408513436255e-05, 'epoch': 0.19}        \n",
      "{'loss': 2.0954, 'learning_rate': 4.765283960191561e-05, 'epoch': 0.19}         \n",
      "{'loss': 3.4633, 'learning_rate': 4.750496992423307e-05, 'epoch': 0.19}         \n",
      "{'loss': 1.2828, 'learning_rate': 4.735282755426004e-05, 'epoch': 0.2}          \n",
      "{'loss': 1.7628, 'learning_rate': 4.719644137706205e-05, 'epoch': 0.2}          \n",
      "{'loss': 1.6152, 'learning_rate': 4.703584108341481e-05, 'epoch': 0.21}         \n",
      "{'loss': 1.0042, 'learning_rate': 4.6871057164167143e-05, 'epoch': 0.21}        \n",
      "{'loss': 1.6946, 'learning_rate': 4.6702120904452256e-05, 'epoch': 0.22}        \n",
      " 21%|███████▌                            | 500/2400 [1:44:15<6:38:13, 12.58s/it][INFO|trainer.py:2926] 2023-08-31 17:54:51,523 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-500\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 17:54:51,627 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 17:54:51,627 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.6329, 'learning_rate': 4.652906437774799e-05, 'epoch': 0.22}         \n",
      "{'loss': 1.1193, 'learning_rate': 4.635192043978756e-05, 'epoch': 0.23}         \n",
      "{'loss': 1.0166, 'learning_rate': 4.6189023939709096e-05, 'epoch': 0.23}        \n",
      "{'loss': 1.4671, 'learning_rate': 4.600420721207053e-05, 'epoch': 0.23}         \n",
      "{'loss': 1.7173, 'learning_rate': 4.581540272019476e-05, 'epoch': 0.24}         \n",
      "{'loss': 2.2066, 'learning_rate': 4.5622646309652794e-05, 'epoch': 0.24}        \n",
      "{'loss': 1.6986, 'learning_rate': 4.542597457630909e-05, 'epoch': 0.25}         \n",
      "{'loss': 1.2986, 'learning_rate': 4.522542485937369e-05, 'epoch': 0.25}         \n",
      "{'loss': 1.193, 'learning_rate': 4.502103523431312e-05, 'epoch': 0.26}          \n",
      "{'loss': 1.7755, 'learning_rate': 4.481284450562163e-05, 'epoch': 0.26}         \n",
      " 25%|█████████                           | 600/2400 [2:05:08<6:14:39, 12.49s/it][INFO|trainer.py:2926] 2023-08-31 18:15:44,481 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-600\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 18:15:44,584 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 18:15:44,584 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 1.9256, 'learning_rate': 4.460089219945382e-05, 'epoch': 0.26}         \n",
      "{'loss': 2.2601, 'learning_rate': 4.438521855612054e-05, 'epoch': 0.27}         \n",
      "{'loss': 1.4167, 'learning_rate': 4.41658645224489e-05, 'epoch': 0.27}          \n",
      "{'loss': 2.4121, 'learning_rate': 4.3942871744008374e-05, 'epoch': 0.28}        \n",
      "{'loss': 1.6086, 'learning_rate': 4.371628255720415e-05, 'epoch': 0.28}         \n",
      "{'loss': 1.538, 'learning_rate': 4.3486139981239304e-05, 'epoch': 0.29}         \n",
      "{'loss': 1.9748, 'learning_rate': 4.325248770994741e-05, 'epoch': 0.29}         \n",
      "{'loss': 2.0964, 'learning_rate': 4.301537010349696e-05, 'epoch': 0.29}         \n",
      "{'loss': 1.2152, 'learning_rate': 4.277483217996941e-05, 'epoch': 0.3}          \n",
      "{'loss': 2.674, 'learning_rate': 4.2530919606812216e-05, 'epoch': 0.3}          \n",
      " 29%|██████████▌                         | 700/2400 [2:25:57<5:54:07, 12.50s/it][INFO|trainer.py:2926] 2023-08-31 18:36:33,567 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-700\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 18:36:33,674 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 18:36:33,674 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-700/special_tokens_map.json\n",
      "{'loss': 2.2806, 'learning_rate': 4.2283678692168615e-05, 'epoch': 0.31}        \n",
      "{'loss': 1.0896, 'learning_rate': 4.203315637608578e-05, 'epoch': 0.31}         \n",
      "{'loss': 1.5469, 'learning_rate': 4.177940022160299e-05, 'epoch': 0.32}         \n",
      "{'loss': 2.4407, 'learning_rate': 4.152245840572153e-05, 'epoch': 0.32}         \n",
      "{'loss': 1.7392, 'learning_rate': 4.126237971025803e-05, 'epoch': 0.32}         \n",
      "{'loss': 1.6823, 'learning_rate': 4.099921351258292e-05, 'epoch': 0.33}         \n",
      "{'loss': 1.6622, 'learning_rate': 4.073300977624594e-05, 'epoch': 0.33}         \n",
      "{'loss': 1.7046, 'learning_rate': 4.046381904149024e-05, 'epoch': 0.34}         \n",
      "{'loss': 1.4654, 'learning_rate': 4.019169241565703e-05, 'epoch': 0.34}         \n",
      "{'loss': 2.7048, 'learning_rate': 3.991668156348261e-05, 'epoch': 0.35}         \n",
      " 33%|████████████                        | 800/2400 [2:46:48<5:33:38, 12.51s/it][INFO|trainer.py:2926] 2023-08-31 18:57:23,933 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-800\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 18:57:24,036 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 18:57:24,036 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 2.0628, 'learning_rate': 3.9638838697289484e-05, 'epoch': 0.35}        \n",
      "{'loss': 2.9284, 'learning_rate': 3.935821656707359e-05, 'epoch': 0.35}         \n",
      "{'loss': 1.9673, 'learning_rate': 3.9074868450489465e-05, 'epoch': 0.36}        \n",
      "{'loss': 1.0606, 'learning_rate': 3.878884814273509e-05, 'epoch': 0.36}         \n",
      "{'loss': 2.0409, 'learning_rate': 3.850020994633868e-05, 'epoch': 0.37}         \n",
      "{'loss': 1.5474, 'learning_rate': 3.8209008660848974e-05, 'epoch': 0.37}        \n",
      "{'loss': 1.6097, 'learning_rate': 3.7915299572431286e-05, 'epoch': 0.38}        \n",
      "{'loss': 1.7742, 'learning_rate': 3.76191384433711e-05, 'epoch': 0.38}          \n",
      "{'loss': 2.2322, 'learning_rate': 3.732058150148729e-05, 'epoch': 0.39}         \n",
      "{'loss': 1.1758, 'learning_rate': 3.7019685429456986e-05, 'epoch': 0.39}        \n",
      " 38%|█████████████▌                      | 900/2400 [3:07:38<5:12:22, 12.50s/it][INFO|trainer.py:2926] 2023-08-31 19:18:14,237 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-900\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 19:18:14,342 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 19:18:14,343 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-900/special_tokens_map.json\n",
      "{'loss': 1.1027, 'learning_rate': 3.671650735405404e-05, 'epoch': 0.39}         \n",
      "{'loss': 1.3272, 'learning_rate': 3.6411104835303166e-05, 'epoch': 0.4}         \n",
      "{'loss': 1.4595, 'learning_rate': 3.610353585555191e-05, 'epoch': 0.4}          \n",
      "{'loss': 2.3427, 'learning_rate': 3.579385880846232e-05, 'epoch': 0.41}         \n",
      "{'loss': 1.3627, 'learning_rate': 3.548213248792467e-05, 'epoch': 0.41}         \n",
      "{'loss': 1.3945, 'learning_rate': 3.516841607689501e-05, 'epoch': 0.42}         \n",
      "{'loss': 1.1403, 'learning_rate': 3.485276913615905e-05, 'epoch': 0.42}         \n",
      "{'loss': 2.7054, 'learning_rate': 3.453525159302415e-05, 'epoch': 0.42}         \n",
      "{'loss': 1.4126, 'learning_rate': 3.4215923729941866e-05, 'epoch': 0.43}        \n",
      "{'loss': 1.2817, 'learning_rate': 3.389484617306292e-05, 'epoch': 0.43}         \n",
      " 42%|██████████████▌                    | 1000/2400 [3:28:27<4:52:24, 12.53s/it][INFO|trainer.py:2926] 2023-08-31 19:39:02,901 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-1000\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 19:39:03,002 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 19:39:03,002 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 2.9058, 'learning_rate': 3.357207988072701e-05, 'epoch': 0.44}         \n",
      "{'loss': 1.639, 'learning_rate': 3.3247686131889574e-05, 'epoch': 0.44}         \n",
      "{'loss': 1.5787, 'learning_rate': 3.2921726514487614e-05, 'epoch': 0.45}        \n",
      "{'loss': 2.6119, 'learning_rate': 3.2594262913746865e-05, 'epoch': 0.45}        \n",
      "{'loss': 2.2649, 'learning_rate': 3.22653575004326e-05, 'epoch': 0.45}          \n",
      "{'loss': 1.0033, 'learning_rate': 3.1935072719046115e-05, 'epoch': 0.46}        \n",
      "{'loss': 1.5058, 'learning_rate': 3.1603471275969335e-05, 'epoch': 0.46}        \n",
      "{'loss': 2.9176, 'learning_rate': 3.127061612755961e-05, 'epoch': 0.47}         \n",
      "{'loss': 2.2854, 'learning_rate': 3.093657046819722e-05, 'epoch': 0.47}         \n",
      "{'loss': 1.7579, 'learning_rate': 3.06013977182874e-05, 'epoch': 0.48}          \n",
      " 46%|████████████████                   | 1100/2400 [3:49:16<4:31:57, 12.55s/it][INFO|trainer.py:2926] 2023-08-31 19:59:52,581 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-1100\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 19:59:52,687 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-1100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 19:59:52,687 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-1100/special_tokens_map.json\n",
      "{'loss': 1.5668, 'learning_rate': 3.0265161512219796e-05, 'epoch': 0.48}        \n",
      "{'loss': 1.7538, 'learning_rate': 2.9927925686287006e-05, 'epoch': 0.48}        \n",
      "{'loss': 2.6218, 'learning_rate': 2.9589754266565002e-05, 'epoch': 0.49}        \n",
      "{'loss': 2.1338, 'learning_rate': 2.925071145675733e-05, 'epoch': 0.49}         \n",
      "{'loss': 2.0505, 'learning_rate': 2.8910861626005776e-05, 'epoch': 0.5}         \n",
      "{'loss': 1.828, 'learning_rate': 2.8570269296669466e-05, 'epoch': 0.5}          \n",
      "{'loss': 1.5493, 'learning_rate': 2.8228999132074985e-05, 'epoch': 0.51}        \n",
      "{'loss': 1.151, 'learning_rate': 2.788711592423966e-05, 'epoch': 0.51}          \n",
      "{'loss': 3.7369, 'learning_rate': 2.754468458157043e-05, 'epoch': 0.51}         \n",
      "{'loss': 1.9932, 'learning_rate': 2.720177011654067e-05, 'epoch': 0.52}         \n",
      " 50%|█████████████████▌                 | 1200/2400 [4:10:08<4:10:42, 12.54s/it][INFO|trainer.py:2926] 2023-08-31 20:20:44,570 >> Saving model checkpoint to ./output/promptcblue-llama-7b-pt-v0/checkpoint-1200\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-08-31 20:20:44,670 >> tokenizer config file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-1200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-08-31 20:20:44,670 >> Special tokens file saved in ./output/promptcblue-llama-7b-pt-v0/checkpoint-1200/special_tokens_map.json\n",
      "{'loss': 2.0611, 'learning_rate': 2.6858437633347194e-05, 'epoch': 0.52}        \n",
      " 51%|█████████████████▋                 | 1215/2400 [4:13:15<4:03:46, 12.34s/it]"
     ]
    }
   ],
   "source": [
    "!sh src/ft_llama_lora/run_train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b6fb3a-e43a-4df3-b6da-6d75e3dac2a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T08:03:52.125143Z",
     "iopub.status.busy": "2023-08-31T08:03:52.124660Z",
     "iopub.status.idle": "2023-08-31T08:03:56.325188Z",
     "shell.execute_reply": "2023-08-31T08:03:56.324483Z",
     "shell.execute_reply.started": "2023-08-31T08:03:52.125112Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting peft==0.4.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/88/a0/6e1c23293a922a9c9e9bd8d56a60cd78ecf531fdabe45ac975e142bfbe86/peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m484.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from peft==0.4.0) (1.22.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from peft==0.4.0) (23.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from peft==0.4.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from peft==0.4.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from peft==0.4.0) (2.0.1+cu117)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (from peft==0.4.0) (4.30.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.8/site-packages (from peft==0.4.0) (0.21.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.8/site-packages (from peft==0.4.0) (0.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.4.0) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.4.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.4.0) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.4.0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0) (3.27.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0) (16.0.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.8/site-packages (from transformers->peft==0.4.0) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers->peft==0.4.0) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers->peft==0.4.0) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers->peft==0.4.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers->peft==0.4.0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->peft==0.4.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->peft==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->peft==0.4.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->peft==0.4.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: peft\n",
      "Successfully installed peft-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d8469-8710-4d7c-ba7e-29d9842e9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅保存lora参数, peftmodel.save_pretrained(\"output_dir\")\n",
    "# 加载多个lora模块：https://blog.csdn.net/BIT_666/article/details/132065177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62ec2fb-a27c-4b33-a902-a65bdae19d38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T02:09:16.545640Z",
     "iopub.status.busy": "2023-09-01T02:09:16.545076Z",
     "iopub.status.idle": "2023-09-01T02:11:40.678832Z",
     "shell.execute_reply": "2023-09-01T02:11:40.678203Z",
     "shell.execute_reply.started": "2023-09-01T02:09:16.545618Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 10:09:22,409] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "2023-09-01 10:09:23.123730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "09/01/2023 10:09:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "[INFO|configuration_utils.py:667] 2023-09-01 10:09:25,203 >> loading configuration file /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-09-01 10:09:25,203 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49954\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-09-01 10:09:25,204 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-09-01 10:09:25,204 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-09-01 10:09:25,204 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-09-01 10:09:25,204 >> loading file tokenizer_config.json\n",
      "09/01/2023 10:09:26 - INFO - datasets.builder - Using custom data configuration default-595c04e6da153d9d\n",
      "09/01/2023 10:09:26 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/json\n",
      "09/01/2023 10:09:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "09/01/2023 10:09:26 - INFO - datasets.info - Loading Dataset info from /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\n",
      "09/01/2023 10:09:26 - WARNING - datasets.builder - Found cached dataset json (/mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "09/01/2023 10:09:26 - INFO - datasets.info - Loading Dataset info from /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 218.23it/s]\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2df00f26b3af8cbc_00000_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2df00f26b3af8cbc_00001_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2df00f26b3af8cbc_00002_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2df00f26b3af8cbc_00003_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Running tokenizer on dataset (num_proc=4):   0%|  | 0/20 [00:00<?, ? examples/s]09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2df00f26b3af8cbc_00001_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2df00f26b3af8cbc_00002_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2df00f26b3af8cbc_00000_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2df00f26b3af8cbc_00003_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-f4142dd6b499af7b_00000_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-f4142dd6b499af7b_00001_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-f4142dd6b499af7b_00002_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-f4142dd6b499af7b_00003_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Running tokenizer on dataset (num_proc=4):   0%|  | 0/20 [00:00<?, ? examples/s]09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-f4142dd6b499af7b_00000_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-f4142dd6b499af7b_00003_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-f4142dd6b499af7b_00001_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-f4142dd6b499af7b_00002_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-470a95906b976566_00000_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-470a95906b976566_00001_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-470a95906b976566_00002_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-470a95906b976566_00003_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Running tokenizer on dataset (num_proc=4):   0%|  | 0/20 [00:00<?, ? examples/s]09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-470a95906b976566_00000_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-470a95906b976566_00003_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-470a95906b976566_00001_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-470a95906b976566_00002_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "tokenized_dataset:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n",
      "[1, 29871, 31658, 30383, 13, 32237, 31138, 32380, 36655, 30214, 39313, 34339, 36655, 30275, 34534, 30210, 33942, 37110, 30214, 31666, 37887, 38920, 31149, 39990, 30952, 30383, 13, 36655, 32342, 30383, 13, 32822, 30383, 32593, 41049, 31639, 32269, 43434, 43095, 30210, 32027, 30882, 13, 32822, 30383, 38020, 33096, 30806, 36021, 32822, 31999, 38020, 32764, 32066, 32161, 32058, 30882, 13, 34339, 36655, 30383, 13, 33062, 30383, 31238, 32292, 30850, 32980, 33673, 34982, 43095, 13, 30682, 31333, 39990, 30952, 41133, 30383, 32009, 42147, 31751, 33942, 30214, 42147, 31751, 33942, 30214, 32395, 32237, 35230, 30333, 32932, 35358, 32262, 42147, 31751, 33942, 13, 33091, 30383, 13, 34339, 36655, 32204, 33942, 33858, 39990, 30952, 33933, 30573, 30383, 13, 43095, 30383, 32009, 42147, 31751, 33942, 2]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 34339, 36655, 32204, 33942, 33858, 39990, 30952, 33933, 30573, 30383, 13, 43095, 30383, 32009, 42147, 31751, 33942, 2]\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-b096e7f4aab099e0_00000_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-b096e7f4aab099e0_00001_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-b096e7f4aab099e0_00002_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-b096e7f4aab099e0_00003_of_00004.arrow\n",
      "09/01/2023 10:09:26 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-b096e7f4aab099e0_*_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Concatenating 4 shards\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4b34986914f2401f_00000_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4b34986914f2401f_00001_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4b34986914f2401f_00002_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4b34986914f2401f_00003_of_00004.arrow\n",
      "09/01/2023 10:09:26 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4b34986914f2401f_*_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Concatenating 4 shards\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4a2f5cded25be188_00000_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4a2f5cded25be188_00001_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4a2f5cded25be188_00002_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4a2f5cded25be188_00003_of_00004.arrow\n",
      "09/01/2023 10:09:26 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-595c04e6da153d9d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4a2f5cded25be188_*_of_00004.arrow\n",
      "09/01/2023 10:09:26 - INFO - datasets.arrow_dataset - Concatenating 4 shards\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "})\n",
      "09/01/2023 10:09:26 - INFO - __main__ - Num eval_samples  11\n",
      "09/01/2023 10:09:26 - INFO - __main__ - training example:\n",
      "09/01/2023 10:09:26 - INFO - __main__ - <s> 问：\n",
      "请问是什么意图类型？\n",
      "癫痫怎么治愈\n",
      "搜索意图选项：治疗方案，病情诊断，指标解读，病因分析，注意事项，功效作用，医疗费用\n",
      "答：\n",
      "治疗方案</s><s>问：\n",
      "请根据下述医疗指南文本，提取诊疗决策树：\n",
      "AVNRT患者@合并低血压者可应用升压药物（如去氧肾上腺素、甲氧明或间羟胺），通过反射性兴奋迷走神经终止心动过速。但老年患者、高血压、急性心肌梗死患者等禁用升压药物。\n",
      "说明：(1)我们将诊疗决策树定义为由条件节点和决策节点组成的二叉树，旨在通过简洁的结构化信息表达指南文本，既要求将文本中的核心实体和关系挖掘出来，也需要将这些信息进行串联，形成一个完整的决策流程；(2)在诊疗决策二叉树中，非叶子节点是条件节点，叶子节点是决策节点。对于条件节点，当条件判断结果为“是”时，将转到左侧子节点进行下一个判断或决策，当条件判断结果为“否”时，将转到右侧子节点进行下一个判断或决策。(3)每个节点输出为一个dict，包含三个字段：(3a)\"role\"，即节点类型，表示节点是一个条件节点(\"C\")或者是决策节点(\"D\")；(3b)\"triples\"，即三元组列表，用来描述诊疗知识或者临床信息的三元组，即条件/决策节点的内容，三元组关系共定义了6类：\"临床表现\", \"治疗药物\", \"用法用量\", \"治疗方案\", \"禁用药物\", \"基本情况\"；(3c)\"logical_rel\"，表示多个三元组之间的逻辑关系（取值为and, or, null，当只有三元组的个数<=1时逻辑关系为 null）。(4)整个诊疗决策树以广度优先策略排列为一个列表。\n",
      "答：\n",
      "根据给定的指南文本抽取的诊疗决策树如下：\n",
      "节点0：role=C；logical_rel=null；triples=[[\"AVNRT患者\", \"临床表现\", \"低血压\"]]\n",
      "节点1：role=D；logical_rel=or；triples=[[\"AVNRT患者\", \"治疗药物\", \"升\n",
      "[INFO|modeling_utils.py:2265] 2023-09-01 10:09:26,738 >> Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "[INFO|modeling_utils.py:2278] 2023-09-01 10:09:26,738 >> The device_map was not initialized.Setting device_map to {'':torch.cuda.current_device()}.If you want to use the model for inference, please set device_map ='auto' \n",
      "[INFO|modeling_utils.py:2575] 2023-09-01 10:09:26,738 >> loading weights file /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1173] 2023-09-01 10:09:26,739 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:577] 2023-09-01 10:09:26,740 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2690] 2023-09-01 10:09:26,911 >> Detected 8-bit loading: activating 8-bit loading for this model\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:20<00:00, 20.07s/it]\n",
      "[INFO|modeling_utils.py:3295] 2023-09-01 10:10:47,524 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3303] 2023-09-01 10:10:47,524 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2023-09-01 10:10:47,528 >> loading configuration file /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3/generation_config.json\n",
      "[INFO|configuration_utils.py:577] 2023-09-01 10:10:47,528 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.9,\n",
      "  \"top_p\": 0.6,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "09/01/2023 10:10:47 - INFO - __main__ - Peft from pre-trained model\n",
      "trainable params: 0 || all params: 6,905,483,264 || trainable%: 0.0\n",
      "[INFO|trainer.py:399] 2023-09-01 10:11:35,026 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "[INFO|trainer.py:407] 2023-09-01 10:11:35,026 >> The model is loaded in 8-bit precision. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check  the examples in https://github.com/huggingface/peft for more details.\n",
      "[INFO|trainer.py:577] 2023-09-01 10:11:35,026 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "09/01/2023 10:11:35 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3200] 2023-09-01 10:11:35,029 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3202] 2023-09-01 10:11:35,029 >>   Num examples = 11\n",
      "[INFO|trainer.py:3205] 2023-09-01 10:11:35,029 >>   Batch size = 1\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  4.34it/s]\n",
      "***** eval metrics *****\n",
      "  eval_loss               =     0.5276\n",
      "  eval_runtime            = 0:00:04.45\n",
      "  eval_samples            =         11\n",
      "  eval_samples_per_second =      2.468\n",
      "  eval_steps_per_second   =      2.468\n",
      "  perplexity              =     1.6949\n"
     ]
    }
   ],
   "source": [
    "!sh src/ft_llama_lora/run_eval.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a2ba06-ada9-438f-ac94-5c23c345fb65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T08:00:01.714700Z",
     "iopub.status.busy": "2023-08-31T08:00:01.714356Z",
     "iopub.status.idle": "2023-08-31T08:00:02.991968Z",
     "shell.execute_reply": "2023-08-31T08:00:02.991319Z",
     "shell.execute_reply.started": "2023-08-31T08:00:01.714675Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip list | grep peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a31922-28dd-4fcc-97b6-d3f5df9f9667",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T15:26:53.517329Z",
     "iopub.status.busy": "2023-09-01T15:26:53.516685Z",
     "iopub.status.idle": "2023-09-02T11:48:27.510271Z",
     "shell.execute_reply": "2023-09-02T11:48:27.509430Z",
     "shell.execute_reply.started": "2023-09-01T15:26:53.517303Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 23:26:56,726] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "2023-09-01 23:26:56.986446: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "09/01/2023 23:26:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "[INFO|configuration_utils.py:667] 2023-09-01 23:26:58,202 >> loading configuration file /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-09-01 23:26:58,202 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49954\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-09-01 23:26:58,202 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-09-01 23:26:58,203 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-09-01 23:26:58,203 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1821] 2023-09-01 23:26:58,203 >> loading file tokenizer_config.json\n",
      "09/01/2023 23:26:59 - INFO - datasets.builder - Using custom data configuration default-ba5e73bf665851bc\n",
      "09/01/2023 23:26:59 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/json\n",
      "09/01/2023 23:26:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "09/01/2023 23:26:59 - INFO - datasets.info - Loading Dataset info from /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\n",
      "09/01/2023 23:26:59 - WARNING - datasets.builder - Found cached dataset json (/mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "09/01/2023 23:26:59 - INFO - datasets.info - Loading Dataset info from /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 950.87it/s]\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5c865f791319f72b_00000_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5c865f791319f72b_00001_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5c865f791319f72b_00002_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5c865f791319f72b_00003_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Running tokenizer on dataset (num_proc=4):   0%|  | 0/20 [00:00<?, ? examples/s]09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5c865f791319f72b_00000_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5c865f791319f72b_00001_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5c865f791319f72b_00002_of_00004.arrow\n",
      "Running tokenizer on dataset (num_proc=4):  50%|▌| 10/20 [00:00<00:00, 96.08 exa09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5c865f791319f72b_00003_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5bda563ddf237899_00000_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5bda563ddf237899_00001_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5bda563ddf237899_00002_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5bda563ddf237899_00003_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Running tokenizer on dataset (num_proc=4):   0%|  | 0/20 [00:00<?, ? examples/s]09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5bda563ddf237899_00000_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5bda563ddf237899_00001_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5bda563ddf237899_00002_of_00004.arrow\n",
      "Running tokenizer on dataset (num_proc=4):  75%|▊| 15/20 [00:00<00:00, 137.71 ex09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5bda563ddf237899_00003_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-444bd2bcb4e5126b_00000_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-444bd2bcb4e5126b_00001_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-444bd2bcb4e5126b_00002_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-444bd2bcb4e5126b_00003_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Running tokenizer on dataset (num_proc=4):   0%| | 0/7656 [00:00<?, ? examples/s09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-444bd2bcb4e5126b_00000_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-444bd2bcb4e5126b_00001_of_00004.arrow\n",
      "Running tokenizer on dataset (num_proc=4):   1%| | 63/7656 [00:00<00:12, 586.06 09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-444bd2bcb4e5126b_00002_of_00004.arrow\n",
      "09/01/2023 23:26:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-444bd2bcb4e5126b_00003_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "tokenized_dataset:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7656\n",
      "    })\n",
      "})\n",
      "[1, 29871, 31658, 30383, 13, 32237, 31138, 32380, 36655, 30214, 39313, 34339, 36655, 30275, 34534, 30210, 33942, 37110, 30214, 31666, 37887, 38920, 31149, 39990, 30952, 30383, 13, 36655, 32342, 30383, 13, 32822, 30383, 32593, 41049, 31639, 32269, 43434, 43095, 30210, 32027, 30882, 13, 32822, 30383, 38020, 33096, 30806, 36021, 32822, 31999, 38020, 32764, 32066, 32161, 32058, 30882, 13, 34339, 36655, 30383, 13, 33062, 30383, 31238, 32292, 30850, 32980, 33673, 34982, 43095, 13, 30682, 31333, 39990, 30952, 41133, 30383, 32009, 42147, 31751, 33942, 30214, 42147, 31751, 33942, 30214, 32395, 32237, 35230, 30333, 32932, 35358, 32262, 42147, 31751, 33942, 13, 33091, 30383, 13]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-defab8a8b2a80499_00000_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-defab8a8b2a80499_00001_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-defab8a8b2a80499_00002_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-defab8a8b2a80499_00003_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Grouping texts in chunks of 512 (num_proc=4):   0%| | 0/20 [00:00<?, ? examples/09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-defab8a8b2a80499_00000_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-defab8a8b2a80499_00003_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-defab8a8b2a80499_00001_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-defab8a8b2a80499_00002_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5babb387c674f545_00000_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5babb387c674f545_00001_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5babb387c674f545_00002_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5babb387c674f545_00003_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Grouping texts in chunks of 512 (num_proc=4):   0%| | 0/20 [00:00<?, ? examples/09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5babb387c674f545_00000_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5babb387c674f545_00001_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5babb387c674f545_00002_of_00004.arrow\n",
      "09/01/2023 23:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5babb387c674f545_00003_of_00004.arrow\n",
      "09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Process #0 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75aca18329e3555b_00000_of_00004.arrow\n",
      "09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Process #1 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75aca18329e3555b_00001_of_00004.arrow\n",
      "09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Process #2 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75aca18329e3555b_00002_of_00004.arrow\n",
      "09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Process #3 will write at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75aca18329e3555b_00003_of_00004.arrow\n",
      "09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Spawning 4 processes\n",
      "Grouping texts in chunks of 512 (num_proc=4):   0%| | 0/7656 [00:00<?, ? example09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75aca18329e3555b_00000_of_00004.arrow\n",
      "09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75aca18329e3555b_00001_of_00004.arrow\n",
      "09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75aca18329e3555b_00002_of_00004.arrow\n",
      "09/01/2023 23:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /mnt/workspace/PromptCBLUE-data/hf_datasets_cache/json/default-ba5e73bf665851bc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75aca18329e3555b_00003_of_00004.arrow\n",
      "09/01/2023 23:27:03 - INFO - datasets.arrow_dataset - Concatenating 4 shards    \n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3070\n",
      "    })\n",
      "})\n",
      "{'input_ids': [1, 29871, 31658, 30383, 13, 33109, 30210, 34269, 32623, 30503, 37779, 38496, 35870, 35571, 34214, 30882, 13, 34269, 32623, 30383, 36120, 30429, 31313, 30743, 44027, 32612, 31313, 13, 37779, 38496, 30383, 32992, 30429, 31313, 44027, 32239, 32612, 31313, 13, 38631, 30383, 32409, 30413, 39611, 32097, 32009, 33738, 32635, 30214, 34149, 39611, 36174, 33738, 32635, 30214, 32211, 39611, 30214, 32409, 39611, 13, 33091, 30383, 13, 1, 29871, 31658, 30383, 13, 30505, 33885, 36655, 30275, 42582, 33885, 32624, 30214, 39733, 34128, 34033, 30214, 33942, 30214, 33790, 32752, 37303, 30214, 34128, 41133, 37110, 30383, 13, 43136, 35886, 34950, 32828, 30214, 30705, 36285, 32183, 43136, 37986, 32055, 13, 33091, 30383, 13, 1, 29871, 31658, 30383, 13, 32237, 36655, 32316, 30214, 42582, 33942, 31666, 31791, 32250, 31149, 39990, 30952, 30383, 13, 36655, 32342, 30383, 13, 32822, 30383, 30417, 33805, 45176, 32003, 42828, 42694, 39307, 43560, 31491, 31356, 33766, 13, 32822, 30383, 43340, 30214, 35029, 30705, 32054, 30406, 36690, 13, 34339, 36655, 30383, 13, 33062, 30383, 33805, 45176, 38323, 37569, 13, 39990, 30952, 38631, 30383, 32009, 42147, 31751, 33942, 30214, 42147, 31751, 33942, 30214, 32395, 32237, 35230, 30333, 32932, 35358, 32262, 42147, 31751, 33942, 13, 33091, 30383, 13, 1, 29871, 31658, 30383, 13, 31658, 36021, 36655, 32342, 30383, 13, 33062, 30383, 30685, 32953, 32992, 34086, 32243, 33982, 32262, 32671, 30882, 13, 32822, 30383, 30919, 31076, 13, 32822, 30383, 32992, 30392, 44572, 35564, 32399, 32027, 30882, 13, 32822, 30383, 32295, 35779, 32206, 32092, 30214, 30672, 32054, 37927, 32593, 32779, 31267, 32161, 33279, 36720, 32026, 30214, 33607, 32593, 33809, 30267, 13, 33062, 30383, 30392, 30210, 30214, 45175, 31390, 30755, 30666, 30743, 31977, 33533, 42215, 33932, 13, 32822, 30383, 37822, 30214, 29896, 30408, 43136, 35969, 30882, 13, 33062, 30383, 34589, 38231, 32278, 30214, 34605, 33396, 30408, 32278, 13, 32822, 30383, 33192, 30214, 32992, 32643, 32074, 33932, 38312, 30882, 13, 33062, 30383, 32271, 32032, 30659, 35639, 30214, 32410, 32852, 35374, 30214, 32471, 32613, 30666, 30743, 40901, 33633, 13, 33062, 30383, 32643, 32937, 13, 32822, 30383, 37822, 30214, 32764, 30392, 35250, 35131, 13, 32822, 30383, 34394, 30392, 32106, 33472, 30214, 37165, 30666, 30743, 40901, 33633, 13, 32822, 30383, 32346, 31999, 32992, 39519, 45062, 13, 33062, 30383, 41254, 32074, 32005, 33697, 30486, 33703, 30210, 13, 32822, 30383, 33192, 30214, 32306, 32074, 33697, 30486, 33703, 13, 33062, 30383, 33192, 30584, 32937, 13, 32822, 30383, 31076, 30214, 13, 32822, 30383, 32593, 30417, 32519, 32026, 32003, 36387, 35876, 32797, 31391, 37451, 30214, 39295, 30780, 30822, 30437, 39793, 33954, 30267, 13, 33062, 30383, 32013, 32243, 32243, 38615, 43204, 30257, 32026, 32055, 13, 32822, 30383, 38615, 30214, 36596, 30257, 32026, 13, 33062, 30383, 42215, 33932, 34105, 32306, 32749, 32027, 30882, 13, 32822, 30383, 37822, 30214, 32306, 32749, 33869, 32183, 13, 33062, 30383, 32937, 30214, 33318, 30584, 13, 32822, 30383, 30413, 31915, 32047, 13, 32822, 30383, 32746, 40901, 30210, 33684, 30214, 32236, 32749, 30780, 34618, 33472, 30743, 30214, 33808, 33072, 32468, 35250, 35131, 13, 33062, 30383, 32133, 32050, 30214, 32937, 30214, 30672, 37085, 32746, 30210, 30214, 33318, 30584, 13, 32822, 30383, 30413, 32912, 30584, 13, 32822, 30383, 35435, 32992, 32500, 33969, 33406, 30584, 13, 33062, 30383, 37840, 31522, 39652, 31658], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n",
      "[INFO|modeling_utils.py:2265] 2023-09-01 23:27:03,206 >> Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "[INFO|modeling_utils.py:2575] 2023-09-01 23:27:03,206 >> loading weights file /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1173] 2023-09-01 23:27:03,206 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:577] 2023-09-01 23:27:03,207 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2690] 2023-09-01 23:27:03,329 >> Detected 8-bit loading: activating 8-bit loading for this model\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.90s/it]\n",
      "[INFO|modeling_utils.py:3295] 2023-09-01 23:27:12,783 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3303] 2023-09-01 23:27:12,783 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2023-09-01 23:27:12,786 >> loading configuration file /mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3/generation_config.json\n",
      "[INFO|configuration_utils.py:577] 2023-09-01 23:27:12,787 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.9,\n",
      "  \"top_p\": 0.6,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "09/01/2023 23:27:12 - INFO - __main__ - Peft from pre-trained model\n",
      "trainable params: 0 || all params: 6,905,483,264 || trainable%: 0.0\n",
      "[INFO|trainer.py:399] 2023-09-01 23:28:00,241 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "[INFO|trainer.py:407] 2023-09-01 23:28:00,242 >> The model is loaded in 8-bit precision. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check  the examples in https://github.com/huggingface/peft for more details.\n",
      "[INFO|trainer.py:577] 2023-09-01 23:28:00,242 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "09/01/2023 23:28:00 - INFO - __main__ - ******************************Predict******************************\n",
      "GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.9,\n",
      "  \"top_p\": 0.6,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "100%|████████████████████████████████████| 7656/7656 [20:20:26<00:00,  9.56s/it]\n",
      "[{'input': '下面的搜索词和页面标签的意思有多相同？\\n搜索词：冬季上火了吃什么降火\\n页面标签：宝宝上火吃什么药降火\\n选项：完全不匹配或者没有参考价值，很少匹配有一些参考价值，部分匹配，完全匹配', 'target': '完全匹配', 'answer_choices': ['完全不匹配或者没有参考价值', '很少匹配有一些参考价值', '部分匹配', '完全匹配'], 'task_type': 'nli', 'task_dataset': 'KUAKE-QTR', 'sample_id': 'test-24460'}, {'input': '在医疗对话中找出医疗操作，具体的药物名称，症状，医学检查检验，药物类别实体：\\n大便中有粘液，化验一下大便常规吧', 'target': '上述句子中的实体包含：\\n医学检查检验实体：大便常规', 'answer_choices': ['医疗操作', '具体的药物名称', '症状', '医学检查检验', '药物类别'], 'task_type': 'ner', 'task_dataset': 'IMCS-V2-NER', 'sample_id': 'test-176126'}]\n"
     ]
    }
   ],
   "source": [
    "!sh src/ft_llama_lora/run_predict.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03766ada-d7f3-4dce-acb8-0b654363b0cb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T02:13:31.788444Z",
     "iopub.status.busy": "2023-09-01T02:13:31.787958Z",
     "iopub.status.idle": "2023-09-01T02:13:31.997506Z",
     "shell.execute_reply": "2023-09-01T02:13:31.996960Z",
     "shell.execute_reply.started": "2023-09-01T02:13:31.788423Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-2400\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1700\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-800\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1400\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1300\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-600\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-2100\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-400\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1100\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-700\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-200\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-100\n",
      "152K\toutput/promptcblue-llama-7b-pt-v0/runs\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1900\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-2300\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-2200\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-300\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1200\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-2000\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1600\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-500\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1500\n",
      "8.0K\toutput/promptcblue-llama-7b-pt-v0/.ipynb_checkpoints\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1800\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-1000\n",
      "230M\toutput/promptcblue-llama-7b-pt-v0/checkpoint-900\n",
      "5.4G\toutput/promptcblue-llama-7b-pt-v0/\n"
     ]
    }
   ],
   "source": [
    "!du -h -d 1 output/promptcblue-llama-7b-pt-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59e88c6-9de4-4c55-b129-df27140476a7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T14:41:27.145987Z",
     "iopub.status.busy": "2023-09-01T14:41:27.145632Z",
     "iopub.status.idle": "2023-09-01T14:41:36.949431Z",
     "shell.execute_reply": "2023-09-01T14:41:36.948851Z",
     "shell.execute_reply.started": "2023-09-01T14:41:27.145963Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20168f32be144b8181e506cf0c17d0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed, DataCollatorForSeq2Seq,\n",
    ")\n",
    "# config_kwargs = {\n",
    "#         \"cache_dir\": model_args.cache_dir,\n",
    "#         \"revision\": model_args.model_revision,\n",
    "#         \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "#     }\n",
    "model_name_or_path = \"./output/promptcblue-llama-7b-pt-v0/checkpoint-1500\"\n",
    "config_path=\"/mnt/workspace/dataroot/models/michaelwzhu/Chinese-LlaMA2-chat-7B-sft-v0.3\"\n",
    "config = AutoConfig.from_pretrained(config_path)#, **config_kwargs)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config_path,\n",
    "    from_tf=False,\n",
    "    config=config,\n",
    "    # cache_dir=model_args.cache_dir,\n",
    "    # revision=model_args.model_revision,\n",
    "    use_auth_token=False,\n",
    "    # torch_dtype=torch_dtype,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")#.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031e7437-7b92-44c0-90c4-173f99c84218",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T14:38:38.609298Z",
     "iopub.status.busy": "2023-09-01T14:38:38.608778Z",
     "iopub.status.idle": "2023-09-01T14:38:38.629416Z",
     "shell.execute_reply": "2023-09-01T14:38:38.628746Z",
     "shell.execute_reply.started": "2023-09-01T14:38:38.609275Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path, padding_side=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960aed6-cb12-4623-aa43-7537bc798508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"dev\": \"/mnt/workspace/PromptCBLUE-data/dev20.jsonl\",\n",
    "        \"test\": \"/mnt/workspace/PromptCBLUE-data/testA20.jsonl\"\n",
    "    },\n",
    "    # cache_dir=\"/mnt/workspace/PromptCBLUE-data/hf_dataset_cache\",\n",
    "    use_auth_token=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a307a121-4a80-46b3-a974-42e8d4cc71d0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T14:40:49.767441Z",
     "iopub.status.busy": "2023-09-01T14:40:49.767097Z",
     "iopub.status.idle": "2023-09-01T14:40:49.772637Z",
     "shell.execute_reply": "2023-09-01T14:40:49.772113Z",
     "shell.execute_reply.started": "2023-09-01T14:40:49.767419Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_column = \"input\"\n",
    "response_column = \"target\"\n",
    "\n",
    "def tokenize_function(example, do_predict=True):\n",
    "    # print(example)\n",
    "    query = example[prompt_column]\n",
    "    response = example[response_column]\n",
    "    prompt = \"<s>问：\\n{}\\n答：\\n\".format(query.strip().replace(\"答：\", \"\").strip())\n",
    "\n",
    "    if do_predict:\n",
    "        result = tokenizer(prompt, padding=\"max_length\", truncation=True,\n",
    "                           max_length=512, add_special_tokens=False)\n",
    "        result[\"labels\"] = [-100] * len(result[\"input_ids\"])\n",
    "        return result\n",
    "    else:\n",
    "        answer = \"{}\\n</s>\".format(response.strip())\n",
    "        tokenized_prompt = tokenizer(prompt, add_special_tokens=False)\n",
    "        user_prompt_len = len(tokenized_prompt[\"input_ids\"])\n",
    "        result = tokenizer(text=prompt + answer, add_special_tokens=False)\n",
    "        result[\"labels\"] = [-100] * user_prompt_len + result[\"input_ids\"][user_prompt_len:]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e779aff6-5712-4917-b358-4ec1e83b90e4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-01T14:41:01.138151Z",
     "iopub.status.busy": "2023-09-01T14:41:01.137795Z",
     "iopub.status.idle": "2023-09-01T14:41:01.478322Z",
     "shell.execute_reply": "2023-09-01T14:41:01.477692Z",
     "shell.execute_reply.started": "2023-09-01T14:41:01.138128Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5afa9558ea42aa9b1b515ee50bb04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5958192a8f5849f5aaabd36281bd8843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_dataset:  DatasetDict({\n",
      "    dev: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 29871, 31658, 30383, 13, 33885, 34269, 30383, 30672, 42714, 29941, 30850, 34636, 36814, 30214, 29896, 30850, 30980, 31975, 30743, 30214, 30437, 34976, 32027, 13, 32465, 32316, 30383, 30919, 31076, 30214, 30919, 29896, 30534, 32406, 30850, 32981, 30936, 36814, 30214, 29906, 30534, 29906, 30850, 38431, 46047, 30980, 31975, 32454, 34300, 30743, 30267, 32307, 31733, 32009, 30952, 32125, 30214, 33539, 29906, 30534, 32352, 30850, 38431, 37864, 34428, 30980, 31975, 30214, 29941, 30534, 32114, 30850, 34647, 34976, 30743, 30214, 31356, 32214, 38954, 30919, 37864, 34428, 30210, 30214, 34419, 30392, 46047, 30210, 30267, 40843, 30503, 46047, 30980, 31975, 33173, 32529, 32225, 31117, 30214, 30392, 32009, 33029, 34976, 30210, 30267, 13, 34131, 34269, 30503, 32465, 32262, 32293, 30882, 13, 38631, 29901, 29871, 32293, 30214, 30413, 32293, 13, 33091, 30383, 13]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=False,\n",
    "            num_proc=4,\n",
    "            remove_columns=[prompt_column, response_column, \"answer_choices\", \"task_type\", \"task_dataset\", \"sample_id\"],\n",
    "            load_from_cache_file=False,\n",
    "            # cache_file_names={k: os.path.join(data_args.dataset_cache_dir, f'tokenized.arrow') for k in raw_datasets},\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "print(\"tokenized_dataset: \", tokenized_dataset)\n",
    "print(tokenized_dataset[\"dev\"][3]['input_ids'])\n",
    "print(tokenized_dataset[\"dev\"][3]['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19a68e65-737e-4117-8520-61e9ae4bc8b2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-08-31T07:04:21.645428Z",
     "iopub.status.busy": "2023-08-31T07:04:21.644737Z",
     "iopub.status.idle": "2023-08-31T07:04:24.508335Z",
     "shell.execute_reply": "2023-08-31T07:04:24.507775Z",
     "shell.execute_reply.started": "2023-08-31T07:04:21.645409Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"辨别下列对话中提及的症状，并判断其阴阳性：\\n对话历史：\\n患者：您怀疑有点支气管炎？\\n医生：如果检查单纯支气管炎可以口服氨溴索，肺力咳\\n当前对话：\\n医生：如果检查有肺炎建议输液治疗\\n症状阴阳性选项如下：没有患有该症状，患有该症状，无法根据上下文确定病人是否患有该症状\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(torch.device('cuda:0'))\n",
    "res = model.generate(**inputs, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198959e8-e784-44dc-a66c-3031d37adb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
